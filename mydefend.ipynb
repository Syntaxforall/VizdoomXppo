{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d05de5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vizdoom in ./myenv/lib/python3.12/site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy in ./myenv/lib/python3.12/site-packages (from vizdoom) (2.3.0)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in ./myenv/lib/python3.12/site-packages (from vizdoom) (1.1.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in ./myenv/lib/python3.12/site-packages (from vizdoom) (2.6.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./myenv/lib/python3.12/site-packages (from gymnasium>=0.28.0->vizdoom) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./myenv/lib/python3.12/site-packages (from gymnasium>=0.28.0->vizdoom) (4.14.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./myenv/lib/python3.12/site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "37bff382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ViZDoom'...\n",
      "remote: Enumerating objects: 19226, done.\u001b[K\n",
      "remote: Counting objects: 100% (2709/2709), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1023/1023), done.\u001b[K\n",
      "remote: Total 19226 (delta 1901), reused 1769 (delta 1652), pack-reused 16517 (from 1)\u001b[K\n",
      "Receiving objects: 100% (19226/19226), 59.18 MiB | 3.65 MiB/s, done.\n",
      "Resolving deltas: 100% (12329/12329), done.\n"
     ]
    }
   ],
   "source": [
    "!cd github & git clone https://github.com/Farama-Foundation/ViZDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da33c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random as r\n",
    "import time as t\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f4bfbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DoomGame()\n",
    "game.load_config('ViZDoom/scenarios/defend_the_center.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76912e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.identity(3,dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c411e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State # 1 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 2 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 3 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 4 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 5 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 6 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 7 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 8 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 9 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 10 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 11 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 12 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 13 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 14 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 15 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 16 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 17 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 18 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 19 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 20 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 21 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 22 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 23 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 24 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 25 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 26 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 27 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 28 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 29 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 30 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 31 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 32 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 33 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 34 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 35 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 36 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 37 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 38 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 39 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 40 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 41 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 42 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 43 Reward: 1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 44 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 45 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 46 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 47 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 48 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 49 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 50 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 51 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 52 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 53 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 54 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 55 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 56 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 57 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 58 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 59 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 60 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 61 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 62 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 63 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 64 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 65 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 66 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 67 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 68 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 69 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 70 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 71 Reward: -1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Episode finished! Total reward: 0.0\n",
      "State # 1 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 2 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 3 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 4 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 5 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 6 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 7 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 8 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 9 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 10 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 11 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 12 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 13 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 14 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 15 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 16 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 17 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 18 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 19 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 20 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 21 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 22 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 23 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 24 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 25 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 26 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 27 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 28 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 29 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 30 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 31 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 32 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 33 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 34 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 35 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 36 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 37 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 38 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 39 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 40 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 41 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 42 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 43 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 44 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 45 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 46 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 47 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 48 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 49 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 50 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 51 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 52 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 53 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 54 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 55 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 56 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 57 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 58 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 59 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 60 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 61 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 62 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 63 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 64 Reward: -1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Episode finished! Total reward: -1.0\n",
      "State # 1 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 2 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 3 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 4 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 5 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 6 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 7 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 8 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 9 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 10 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 11 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 12 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 13 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 14 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 15 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 16 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 17 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 18 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 19 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 20 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 21 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 22 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 23 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 24 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 25 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 26 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 27 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 28 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 29 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 30 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 31 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 32 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 33 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 34 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 35 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 36 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 37 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 38 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 39 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 40 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 41 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 42 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 43 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 44 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 45 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 46 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 47 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 48 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 49 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 50 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 51 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 52 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 53 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 54 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 55 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 56 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 57 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 58 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 59 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 60 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 61 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 62 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 63 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 64 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 65 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 66 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 67 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 68 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 69 Reward: -1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Episode finished! Total reward: -1.0\n",
      "State # 1 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 2 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 3 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 4 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 5 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 6 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 7 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 8 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 9 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 10 Reward: 1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 11 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 12 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 13 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 14 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 15 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 16 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 17 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 18 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 19 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 20 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 21 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 22 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 23 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 24 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 25 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 26 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 27 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 28 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 29 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 30 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 31 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 32 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 33 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 34 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 35 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 36 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 37 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 38 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 39 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 40 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 41 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 42 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 43 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 44 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 45 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 46 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 47 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 48 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 49 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 50 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 51 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 52 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 53 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 54 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 55 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 56 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 57 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 58 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 59 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 60 Reward: 1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 61 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 62 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 63 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 64 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 65 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 66 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 67 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 68 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 69 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 70 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 71 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 72 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 73 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 74 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 75 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 76 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 77 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 78 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 79 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 80 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 81 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 82 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 83 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 84 Reward: -1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Episode finished! Total reward: 1.0\n",
      "State # 1 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 2 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 3 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 4 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 5 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 6 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 7 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 8 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 9 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 10 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 11 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 12 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 13 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 14 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 15 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 16 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 17 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 18 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 19 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 20 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 21 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 22 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 23 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 24 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 25 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 26 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 27 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 28 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 29 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 30 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 31 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 32 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 33 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 34 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 35 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 36 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 37 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 38 Reward: 1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 39 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 40 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 41 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 42 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 43 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 44 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 45 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 46 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 47 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 48 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 49 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 50 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 51 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 52 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 53 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 54 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 55 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 56 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 57 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 58 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 59 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 60 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 61 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 62 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 63 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 64 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 65 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 66 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 67 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 68 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 69 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 70 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 71 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 72 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 73 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 74 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 75 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 76 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 77 Reward: -1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Episode finished! Total reward: 0.0\n",
      "State # 1 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 2 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 3 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 4 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 5 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 6 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 7 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 8 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 9 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 10 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 11 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 12 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 13 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 14 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 15 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 16 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 17 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 18 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 19 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 20 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 21 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 22 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 23 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 24 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 25 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 26 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 27 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 28 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 29 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 30 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 31 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 32 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 33 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 34 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 35 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 36 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 37 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 38 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 39 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 40 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 41 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 42 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 43 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 44 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 45 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 46 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 47 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 48 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 49 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 50 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 51 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 52 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 53 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 54 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 55 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 56 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 57 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 58 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 59 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 60 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 61 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 62 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 63 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 64 Reward: -1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Episode finished! Total reward: -1.0\n",
      "State # 1 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 2 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 3 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 4 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 5 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 6 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 7 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 8 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 9 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 10 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 11 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 12 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 13 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 14 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 15 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 16 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 17 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 18 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 19 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 20 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 21 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 22 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 23 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 24 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 25 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 26 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 27 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 28 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 29 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 30 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 31 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 32 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 33 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 34 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 35 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 36 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 37 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 38 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 39 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 40 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 41 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 42 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 43 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 44 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 45 Reward: 1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 46 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 47 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 48 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 49 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 50 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 51 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 52 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 53 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 54 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 55 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 56 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 57 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 58 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 59 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 60 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 61 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 62 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 63 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 64 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 65 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 66 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 67 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 68 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 69 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 70 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 71 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 72 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 73 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 74 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 75 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 76 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 77 Reward: -1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Episode finished! Total reward: 0.0\n",
      "State # 1 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 2 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 3 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 4 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 5 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 6 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 7 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 8 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 9 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 10 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 11 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 12 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 13 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 14 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 15 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 16 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 17 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 18 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 19 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 20 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 21 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 22 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 23 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 24 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 25 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 26 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 27 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 28 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 29 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 30 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 31 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 32 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 33 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 34 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 35 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 36 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 37 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 38 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 39 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 40 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 41 Reward: 1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 42 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 43 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 44 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 45 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 46 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 47 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 48 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 49 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 50 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 51 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 52 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 53 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 54 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 55 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 56 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 57 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 58 Reward: 1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 59 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 60 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 61 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 62 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 63 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 64 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 65 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 66 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 67 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 68 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 69 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 70 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 71 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 72 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 73 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 74 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 75 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 76 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 77 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 78 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 79 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 80 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 81 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 82 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 83 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 84 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 85 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 86 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 87 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 88 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 89 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 90 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 91 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 92 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 93 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 94 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 95 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 96 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 97 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 98 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 99 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 100 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 101 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 102 Reward: 1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 103 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 104 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 105 Reward: -1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Episode finished! Total reward: 2.0\n",
      "State # 1 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 2 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 3 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 4 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 5 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 6 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 7 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 8 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 9 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 10 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 11 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 12 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 13 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 14 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 15 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 16 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 17 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 18 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 19 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 20 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 21 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 22 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 23 Reward: 1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 24 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 25 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 26 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 27 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 28 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 29 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 30 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 31 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 32 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 33 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 34 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 35 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 36 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 37 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 38 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 39 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 40 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 41 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 42 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 43 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 44 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 45 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 46 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 47 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 48 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 49 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 50 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 51 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 52 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 53 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 54 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 55 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 56 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 57 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 58 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 59 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 60 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 61 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 62 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 63 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 64 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 65 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 66 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 67 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 68 Reward: -1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Episode finished! Total reward: 0.0\n",
      "State # 1 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 2 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 3 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 4 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 5 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 6 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 7 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 8 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 9 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 10 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 11 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 12 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 13 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 14 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 15 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 16 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 17 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 18 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 19 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 20 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 21 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 22 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 23 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 24 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 25 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 26 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 27 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 28 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 29 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 30 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 31 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 32 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 33 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 34 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 35 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 36 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 37 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 38 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 39 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 40 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 41 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 42 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 43 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 44 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 45 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 46 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 47 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 48 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 49 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 50 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 51 Reward: 1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 52 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 53 Reward: 0.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "State # 54 Reward: -1.0 Action: [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Episode finished! Total reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    game.new_episode()\n",
    "    while not game.is_episode_finished():\n",
    "        state = game.get_state()\n",
    "        img = state.screen_buffer\n",
    "        info = state.game_variables\n",
    "        reward = game.make_action(r.choice(actions),4)\n",
    "        print(\"State #\", state.number, \"Reward:\", reward, \"Action:\", actions)\n",
    "        t.sleep(0.02)\n",
    "    print(\"Episode finished! Total reward:\", game.get_total_reward())\n",
    "    t.sleep(2)\n",
    "    # game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d59019",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9bd1b865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in ./myenv/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in ./myenv/lib/python3.12/site-packages (from gym) (2.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./myenv/lib/python3.12/site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in ./myenv/lib/python3.12/site-packages (from gym) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "77de5d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in ./myenv/lib/python3.12/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: gym in ./myenv/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: vizdoom in ./myenv/lib/python3.12/site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./myenv/lib/python3.12/site-packages (from opencv-python) (2.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./myenv/lib/python3.12/site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in ./myenv/lib/python3.12/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in ./myenv/lib/python3.12/site-packages (from vizdoom) (1.1.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in ./myenv/lib/python3.12/site-packages (from vizdoom) (2.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./myenv/lib/python3.12/site-packages (from gymnasium>=0.28.0->vizdoom) (4.14.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./myenv/lib/python3.12/site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python gym vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "30334cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ViZDoom' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!cd github & git clone https://github.com/Farama-Foundation/ViZDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "147a5dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.18.0 in ./myenv/lib/python3.12/site-packages (from gym) (2.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./myenv/lib/python3.12/site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in ./myenv/lib/python3.12/site-packages (from gym) (0.0.8)\n",
      "Installing collected packages: gym\n",
      "Successfully installed gym-0.26.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "224e92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random as r\n",
    "import time as t\n",
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f413d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import cv2\n",
    "from vizdoom import DoomGame\n",
    "\n",
    "class VizDoomEnv(Env):\n",
    "    def __init__(self, render=False):\n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(\"ViZDoom/scenarios/defend_the_center.cfg\")\n",
    "        self.game.set_window_visible(render)\n",
    "        self.game.init()\n",
    "\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100, 160, 1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    def step(self, action):\n",
    "        actions = np.identity(3)\n",
    "        reward = self.game.make_action(actions[action], 4)\n",
    "        done = self.game.is_episode_finished()\n",
    "\n",
    "        if self.game.get_state():\n",
    "            state = self.grayscale(self.game.get_state().screen_buffer)\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "            info = {\"ammo\": ammo}\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = {\"ammo\": 0}\n",
    "\n",
    "        return state, reward, done, False, info \n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state), {}\n",
    "\n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC)\n",
    "        return np.reshape(resized, (100, 160, 1))\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8afa2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "env= VizDoomEnv(render=True)\n",
    "# print(\"Observation space:\", env.observation_space.sample().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c04446a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "66a30f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78fb2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78359c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d05edf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e094cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in ./myenv/lib/python3.12/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in ./myenv/lib/python3.12/site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in ./myenv/lib/python3.12/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./myenv/lib/python3.12/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./myenv/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in ./myenv/lib/python3.12/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in ./myenv/lib/python3.12/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in ./myenv/lib/python3.12/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in ./myenv/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in ./myenv/lib/python3.12/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./myenv/lib/python3.12/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in ./myenv/lib/python3.12/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in ./myenv/lib/python3.12/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in ./myenv/lib/python3.12/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in ./myenv/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in ./myenv/lib/python3.12/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in ./myenv/lib/python3.12/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: numpy in ./myenv/lib/python3.12/site-packages (from torchvision) (2.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./myenv/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "687c40fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in ./myenv/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (1.1.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (2.3.0)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (2.7.1+cu118)\n",
      "Requirement already satisfied: cloudpickle in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (3.1.1)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (2.3.0)\n",
      "Requirement already satisfied: matplotlib in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (3.10.3)\n",
      "Requirement already satisfied: opencv-python in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (4.11.0.86)\n",
      "Requirement already satisfied: pygame in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (2.19.0)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (7.0.0)\n",
      "Requirement already satisfied: tqdm in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (4.67.1)\n",
      "Requirement already satisfied: rich in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (14.0.0)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (0.11.1)\n",
      "Requirement already satisfied: pillow in ./myenv/lib/python3.12/site-packages (from stable-baselines3[extra]) (11.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./myenv/lib/python3.12/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (4.14.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./myenv/lib/python3.12/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./myenv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./myenv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.73.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./myenv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.8)\n",
      "Requirement already satisfied: packaging in ./myenv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./myenv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (6.31.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./myenv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (80.9.0)\n",
      "Requirement already satisfied: six>1.9 in ./myenv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./myenv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./myenv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in ./myenv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.3.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./myenv/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./myenv/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./myenv/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./myenv/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./myenv/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./myenv/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.12/site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.12/site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./myenv/lib/python3.12/site-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./myenv/lib/python3.12/site-packages (from rich->stable-baselines3[extra]) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./myenv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./myenv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b75d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving models\n",
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47a85775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d1ca6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_defend'\n",
    "LOG_DIR = './logs/log_defend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4899339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89a385f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "env = VizDoomEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "782ecd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gani/Desktop/game/myenv/lib/python3.12/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"CnnPolicy\", env, verbose=1, tensorboard_log=LOG_DIR , learning_rate=0.0001, n_steps=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1947d8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/log_defend/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.6     |\n",
      "|    ep_rew_mean     | 0.347    |\n",
      "| time/              |          |\n",
      "|    fps             | 120      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 84.4        |\n",
      "|    ep_rew_mean          | 0.458       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009174194 |\n",
      "|    clip_fraction        | 0.0361      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.0153     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0523      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00406    |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 92.1        |\n",
      "|    ep_rew_mean          | 0.97        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 196         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008899677 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0264      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | 1.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012390429 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0301      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 109         |\n",
      "|    ep_rew_mean          | 2.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 414         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013218067 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.53        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.023      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 116         |\n",
      "|    ep_rew_mean          | 2.93        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 514         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017449861 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.994      |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0232     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0373     |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 122         |\n",
      "|    ep_rew_mean          | 3.56        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 592         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019281313 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00501     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 125        |\n",
      "|    ep_rew_mean          | 4.16       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 48         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 669        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02077252 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.922     |\n",
      "|    explained_variance   | 0.739      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0363    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 128         |\n",
      "|    ep_rew_mean          | 4.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 743         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021148175 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0434     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 135         |\n",
      "|    ep_rew_mean          | 5.22        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 806         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024542792 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.847      |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0313     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 137         |\n",
      "|    ep_rew_mean          | 5.56        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 866         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025763221 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.831      |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0247     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.049      |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 142         |\n",
      "|    ep_rew_mean          | 6.03        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 926         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025793258 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.793      |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0412     |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 147        |\n",
      "|    ep_rew_mean          | 6.51       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 986        |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02441515 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.801     |\n",
      "|    explained_variance   | 0.864      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0417    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0415    |\n",
      "|    value_loss           | 0.151      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 155        |\n",
      "|    ep_rew_mean          | 7.13       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 1050       |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02937724 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.77      |\n",
      "|    explained_variance   | 0.864      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0415    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0477    |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 163         |\n",
      "|    ep_rew_mean          | 7.71        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 1132        |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027279768 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.724      |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0136     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 171         |\n",
      "|    ep_rew_mean          | 8.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 1198        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026557198 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 178         |\n",
      "|    ep_rew_mean          | 8.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 1268        |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028061884 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0236     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 181         |\n",
      "|    ep_rew_mean          | 9.06        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 1333        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028959427 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00626     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 181        |\n",
      "|    ep_rew_mean          | 9.01       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 1404       |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03110234 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.642     |\n",
      "|    explained_variance   | 0.938      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0311    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 185        |\n",
      "|    ep_rew_mean          | 9.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 1472       |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02674906 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.625     |\n",
      "|    explained_variance   | 0.945      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.026     |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0417    |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 183        |\n",
      "|    ep_rew_mean          | 9.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 1540       |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03281648 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.602     |\n",
      "|    explained_variance   | 0.945      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0056    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0412    |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | 9.1         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 1610        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035490654 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0322      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 178        |\n",
      "|    ep_rew_mean          | 9.1        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 1678       |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03105018 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.63      |\n",
      "|    explained_variance   | 0.94       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0107    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 173         |\n",
      "|    ep_rew_mean          | 8.88        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1734        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035271015 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0249     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0438     |\n",
      "|    value_loss           | 0.0985      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 170        |\n",
      "|    ep_rew_mean          | 8.57       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 1802       |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03779605 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.599     |\n",
      "|    explained_variance   | 0.953      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0224    |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    value_loss           | 0.1        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f7b9ef3a630>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "865eefb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc3bfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= PPO.load('./train/train_defend/best_model_100000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae45e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomEnv(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f57700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gani/Desktop/game/myenv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e77aeda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "657d4226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Action: 2, Reward: -1.0, Info: {'ammo': 0}\n",
      "Episode: 1, Action: 2, Reward: -1.0, Info: {'ammo': 0}\n",
      "Episode: 2, Action: 0, Reward: -1.0, Info: {'ammo': 0}\n",
      "Episode: 3, Action: 1, Reward: -1.0, Info: {'ammo': 0}\n",
      "Episode: 4, Action: 0, Reward: -1.0, Info: {'ammo': 0}\n",
      "Episode: 5, Action: 1, Reward: -1.0, Info: {'ammo': 0}\n",
      "Episode: 6, Action: 0, Reward: -1.0, Info: {'ammo': 0}\n",
      "Episode: 7, Action: 2, Reward: -1.0, Info: {'ammo': 0}\n",
      "Episode: 8, Action: 0, Reward: -1.0, Info: {'ammo': 0}\n",
      "Episode: 9, Action: 1, Reward: -1.0, Info: {'ammo': 0}\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        t.sleep(0.5)\n",
    "        total_reward += reward\n",
    "    print(f\"Episode: {episode}, Action: {action}, Reward: {reward}, Info: {info}\")\n",
    "    t.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37364d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c715e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
